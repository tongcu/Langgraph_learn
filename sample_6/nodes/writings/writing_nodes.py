from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.types import Command
from pydantic import BaseModel, Field
from typing import Optional, Union, Any, List, Dict
from LLM.llm import get_llm
# from tools.client_tool import tools
import logging
import re
import json
from datetime import datetime

Default_model_name = "local_qwen"

def _parse_json_from_content(content):
    # æ¸…ç†å‰åç©ºæ ¼
    content = content.strip()
    
    # ç­–ç•¥ 1ï¼šå°è¯•åŒ¹é… Markdown JSON ä»£ç å—
    json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group(1))
        except json.JSONDecodeError:
            pass # å¦‚æœä»£ç å—é‡Œä¹Ÿæ˜¯åçš„ï¼Œå°è¯•ç­–ç•¥ 2

    # ç­–ç•¥ 2ï¼šå°è¯•æå–æœ€å¤–å±‚çš„ { ... } æˆ– [ ... ]
    # è¿™ç§æ–¹æ³•å¯ä»¥è¿‡æ»¤æ‰ LLM åœ¨ JSON å‰ååŠ çš„åºŸè¯
    structure_match = re.search(r'(\{.*\}|\[.*\])', content, re.DOTALL)
    if structure_match:
        try:
            return json.loads(structure_match.group(1))
        except json.JSONDecodeError:
            pass

    # ç­–ç•¥ 3ï¼šæœ€åçš„æŒ£æ‰ï¼Œç›´æ¥å°è¯•è§£æå…¨æ–‡
    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        logging.error(f"è§£æå¤±è´¥ã€‚LLM è¿”å›çš„å†…å®¹: {content}")
        raise ValueError(f"æ— æ³•ä» LLM å“åº”ä¸­æå–æœ‰æ•ˆçš„ JSON: {e}")


async def outline_node(state, config: RunnableConfig):
    # 1. æå– configurable éƒ¨åˆ†ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—å…¸ï¼‰
    logging.info("--- call_outline_node å¤§çº²ç”ŸæˆèŠ‚ç‚¹ ---")
    configurable = config.get("configurable", {})
    m_name = configurable.get("model_name", Default_model_name) 
    llm = get_llm(model=m_name)

    """å¤§çº²ç”ŸæˆèŠ‚ç‚¹"""
    
    try:
        logging.info("è¿›å…¥å¤§çº²èŠ‚ç‚¹")
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤§çº²
        if state.get("outline_generated", False) and state.get("outline"):
            logging.info("å¤§çº²å·²å­˜åœ¨ï¼Œè·³è¿‡ç”Ÿæˆ")
            state["next_step"] = "call_task_coordinator"
            return state
        
        # # ä»å…¨å±€å˜é‡è·å–æ¨¡å‹
        # from Workflow.workflow import llm
        # if llm is None:
        #     raise ValueError("LLMæ¨¡å‹æœªåˆå§‹åŒ–")
        
        # ç”Ÿæˆå¤§çº²
        from Prompts.prompts import outline_prompt
        from Prompts.writing_styles import get_style_prompt_enhancement, normalize_style
        
        # æ ‡å‡†åŒ–é£æ ¼å¹¶è·å–å¢å¼ºä¿¡æ¯
        normalized_style = normalize_style(state.get("style", "technical"))
        style_enhancement = get_style_prompt_enhancement(normalized_style)
        chapter_count = state.get("chapter_count",5)


        # 2. ç¡®å®š Topic çš„ä¼˜å…ˆçº§é€»è¾‘
        # å°è¯•ç›´æ¥ä» state è·å–
        topic = state.get("topic")
        
        # å¦‚æœ topic ä¸ºç©ºï¼Œå°è¯•ä»æœ€åä¸€æ¡æ¶ˆæ¯æå–
        if not topic or str(topic).strip() == "":
            messages = state.get("messages", [])
            # ä»åå¾€å‰æ‰¾ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            for msg in reversed(messages):
                # å…¼å®¹å­—å…¸æ ¼å¼æˆ– LangChain æ¶ˆæ¯å¯¹è±¡
                content = msg.get("content") if isinstance(msg, dict) else getattr(msg, "content", "")
                role = msg.get("role") if isinstance(msg, dict) else (
                    "user" if "User" in str(type(msg)) else "other"
                )
                
                if content and (role == "user" or "Human" in str(type(msg))):
                    topic = content
                    logging.info(f"Topic ä¸ºç©ºï¼Œå·²ä»å†å²æ¶ˆæ¯ä¸­æ•è·ä¸»é¢˜: {topic[:30]}...")
                    break
        # 3. å¦‚æœä¾ç„¶ä¸ºç©ºï¼Œåˆ™éœ€è¦ä¸­æ–­å¹¶è¯·æ±‚è¾“å…¥
        if not topic:
            logging.warning("æœªèƒ½è·å–åˆ°ä»»ä½•ä¸»é¢˜(Topic)")
            return {
                "next_step": "end", # æˆ–è€…è·³è½¬åˆ°ä¸€ä¸ªä¸“é—¨çš„äººæœºäº¤äº’èŠ‚ç‚¹
                "messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°å†™ä½œä¸»é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘æƒ³å†™ä»€ä¹ˆã€‚")]
            }

        prompt = outline_prompt.format(
            task=topic,
            chapter_count=chapter_count,
            style_enhancement=style_enhancement
        )
        
        # import pdb; pdb.set_trace()
        
        logging.info("æ­£åœ¨ç”Ÿæˆå¤§çº²")
        # response = llm.invoke(prompt)
        response = await llm.ainvoke(prompt)
        
        # è®°å½•LLMé¢„æµ‹
        content = response.content.strip()
        
        # è§£æå¤§çº²
        
        # æå–JSONéƒ¨åˆ†
        outline = _parse_json_from_content(content)
        
        # ä¿å­˜çŠ¶æ€ TBD
        # save_state(state) 
        
        # # æ·»åŠ æ¶ˆæ¯
        # outline_str = json.dumps(outline, ensure_ascii=False, indent=2)
        # # state["next_step"] = "call_task_coordinator"
        return {
            "outline": outline,
            "outline_generated": True,
            "messages": [AIMessage(content=f"å¤§çº²ç”ŸæˆæˆåŠŸ:\n```json\n{json.dumps(outline, ensure_ascii=False, indent=2)}\n```")],
            "last_successful_step": "outline"
        }

    except Exception as e:
        logging.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        return _handle_outline_error(state, e)


# 1. å®šä¹‰ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
class PlanResponse(BaseModel):
    """è§„åˆ’å†³ç­–æ¨¡å‹"""
    status: str = Field(description="å†³ç­–çŠ¶æ€ï¼š'COMPLETE' (ä¿¡æ¯è¶³å¤Ÿ) æˆ– 'INCOMPLETE' (éœ€è¦è¿½é—®)")
    topic: Optional[str] = Field(None, description="ç¡®å®šçš„æœ€ç»ˆæ ‡é¢˜/ä¸»é¢˜")
    chapter_count: Optional[int] = Field(None, description="å»ºè®®çš„ç« èŠ‚æ•°é‡", ge=3, le=10)
    ai_response: str = Field(description="å¦‚æœæ˜¯INCOMPLETEï¼Œè¿™æ˜¯è¿½é—®çš„è¯æœ¯ï¼›å¦‚æœæ˜¯COMPLETEï¼Œè¿™æ˜¯ç¡®è®¤çš„è¯æœ¯")

async def plan_node(state, config: RunnableConfig):
    logging.info("--- [Plan Node] å¼€å§‹è§„åˆ’å†³ç­– ---")


    # è·å– LLM å¹¶ç»‘å®šç»“æ„åŒ–è¾“å‡º
    m_name = config.get("configurable", {}).get("model_name", "gpt-4o")
    base_llm = get_llm(model=m_name)
    
    # æ ¸å¿ƒï¼šä½¿ç”¨ with_structured_output ç¡®ä¿è¾“å‡ºç¬¦åˆ PlanResponse ç±»
    structured_llm = base_llm.with_structured_output(PlanResponse)
    
    messages = state.get("messages", [])
    
    # 2. æ„é€  System Prompt å¼•å¯¼ LLM è¿›è¡Œå†³ç­–
    system_msg = {
        "role": "system",
        "content": (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œè§„åˆ’åŠ©æ‰‹ã€‚ä½ çš„ç›®æ ‡æ˜¯ç¡®å®šã€å†™ä½œä¸»é¢˜ã€‘å’Œã€ç« èŠ‚æ•°é‡ã€‘ã€‚\n"
            "1. å®¡æŸ¥å¯¹è¯å†å²ã€‚å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®ä¸»é¢˜ï¼Œè¯·è®¾æ³•å¼•å¯¼ä»–ã€‚\n"
            "2. å¦‚æœç”¨æˆ·ç»™äº†ä¸»é¢˜ä½†æ²¡ç»™ç« èŠ‚æ•°ï¼Œè¯·æ ¹æ®ä¸»é¢˜æ·±åº¦å»ºè®®ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯5ç« ï¼‰ã€‚\n"
            "3. åªæœ‰å½“ä½ è®¤ä¸ºã€ä¸»é¢˜ã€‘å’Œã€ç« èŠ‚æ•°ã€‘éƒ½å·²æ˜ç¡®ä¸”åˆç†æ—¶ï¼Œstatus æ‰è®¾ä¸º 'COMPLETE'ã€‚"
        )
    }

    # 3. è°ƒç”¨æ¨¡å‹
    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ await å¾—åˆ°çš„æ˜¯ä¸€ä¸ª PlanResponse å¯¹è±¡
    try:
        plan_result: PlanResponse = await structured_llm.ainvoke([system_msg] + messages)
    except Exception as e:
        logging.error(f"ç»“æ„åŒ–æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # æç«¯æƒ…å†µä¸‹çš„æ‰‹åŠ¨è§£æå…œåº•ï¼ˆå¯é€‰ï¼‰
        return {"messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨è§„åˆ’ç³»ç»Ÿæœ‰ç‚¹å¿™ï¼Œè¯·å†è¯•ä¸€æ¬¡ã€‚")]}

    # 4. æ ¹æ®æ¨¡å‹å†³ç­–ä½¿ç”¨ Command è¿›è¡Œè·¯ç”±
    if plan_result.status == "COMPLETE":
        logging.info(f"è§„åˆ’è¾¾æˆä¸€è‡´: {plan_result.topic}")
        
        # ä½¿ç”¨ Command ç›´æ¥è·³è½¬åˆ° outline_node
        return Command(
            update={
                "topic": plan_result.topic,
                "chapter_count": plan_result.chapter_count,
                "messages": [AIMessage(content=plan_result.ai_response)]
            },
            goto="outline_node"
        )
    
    else:
        # ä¿¡æ¯ä¸è¶³ï¼Œç•™åœ¨å½“å‰èŠ‚ç‚¹ï¼Œç­‰å¾…ç”¨æˆ·åœ¨ä¸‹ä¸€è½®å¯¹è¯ä¸­è¾“å…¥
        logging.info("ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­å¯¹è¯...")
        return {
            "messages": [AIMessage(content=plan_result.ai_response)]
        }


# async def plan_node(state, config: RunnableConfig):
#     """ä¸“é—¨è´Ÿè´£æ ¹æ®æ£€ç´¢å†…å®¹è¿›è¡Œå†™ä½œçš„èŠ‚ç‚¹"""
#     from Workflow.workflow import llm
#     from Prompts.prompts import writing_prompt
    
#     curr_idx = state["current_chapter"]
#     chapter_info = state["outline"][curr_idx] if curr_idx < len(state["outline"]) else {"title": f"ç¬¬{curr_idx+1}ç« ", "description": ""}
    
#     # æ„å»º Prompt (ä¿æŒä½ åŸæœ‰çš„é€»è¾‘ï¼Œä½†æ›´ç®€æ´)
#     prompt = writing_prompt.format(
#         task=state["task"],
#         chapter_title=chapter_info["title"],
#         chapter_description=chapter_info.get("description", ""),
#         knowledge_content=state.get("knowledge_content", ""),
#         previous_chapters="\n\n".join(state.get("chapters", []))[-2000:], # åªå–æœ€è¿‘å†…å®¹é˜²è¶…é•¿
#         style_enhancement=state.get("style", "academic"),
#         word_count=1000, # ç¤ºä¾‹
#         unit="å­—"
#     )

#     response = llm.invoke(prompt)
#     content = response.content.strip()
    
#     return {
#         "chapters": [content], # æ³¨æ„è¿™é‡Œæ˜¯ listï¼Œå› ä¸ºä½¿ç”¨äº† operator.add
#         "current_chapter": curr_idx + 1,
#         "messages": [{"role": "assistant", "content": f"ç¬¬{curr_idx+1}ç« ç”Ÿæˆå®Œæˆ"}]
#     }


async def retrieval_node(state, config: RunnableConfig):
    """çŸ¥è¯†æ£€ç´¢èŠ‚ç‚¹ï¼šæ ¹æ®å¤§çº²å’Œå½“å‰è¿›åº¦ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹"""
    logging.info(f"--- ğŸ” [Retrieval Node] æ£€ç´¢ç¬¬ {state.get('current_chapter', 0) + 1} ç« ç›¸å…³çŸ¥è¯† ---")
    
    # 1. æå–åŸºç¡€å‚æ•°
    use_knowledge = state.get("use_knowledge", False)
    knowledge_base = state.get("knowledge_base")
    curr_idx = state.get("current_chapter", 0)
    outline = state.get("outline", [])
    topic = state.get("topic", "")
    
    # å¦‚æœä¸ä½¿ç”¨çŸ¥è¯†åº“æˆ–æœªæŒ‡å®šçŸ¥è¯†åº“ï¼Œç›´æ¥è·³è¿‡
    if not use_knowledge or not knowledge_base:
        logging.info("æœªä½¿ç”¨çŸ¥è¯†åº“æˆ–æœªæŒ‡å®šçŸ¥è¯†åº“ï¼Œè·³è¿‡æ£€ç´¢ç¯èŠ‚")
        return {
            "knowledge_content": "",
            "last_successful_step": "retrieval_skipped"
        }

    # 2. å‡†å¤‡æ£€ç´¢ä¿¡æ¯
    chapter_info = outline[curr_idx] if curr_idx < len(outline) else {}
    chapter_title = chapter_info.get("title", f"ç¬¬{curr_idx + 1}ç« ")
    chapter_description = chapter_info.get("description", "")
    
    # æ„é€ æ£€ç´¢æŸ¥è¯¢è¯­å¥
    search_query = f"{topic} {chapter_title} {chapter_description}"
    
    # è·å–æ£€ç´¢é…ç½®å‚æ•°ï¼ˆä» state ä¸­è·å–ï¼Œæˆ–è€…ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    search_mode = state.get('search_mode', 'hybrid')
    search_k = state.get('search_k', 5)
    score_threshold = state.get('score_threshold', 0.3)
    
    try:
        # 3. åŠ¨æ€å¯¼å…¥çŸ¥è¯†åº“ç®¡ç†å™¨
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ KnowledgeManager æ–‡ä»¶å¤¹å·²å­˜åœ¨äºé¡¹ç›®ä¸­
        from KnowledgeManager.KnowledgeManagerFactory import KnowledgeManagerFactory
        
        logging.info(f"æ­£åœ¨ä½¿ç”¨çŸ¥è¯†åº“ '{knowledge_base}' è¿›è¡Œ {search_mode} æ£€ç´¢...")
        km = KnowledgeManagerFactory.create_knowledge_manager(knowledge_base_name=knowledge_base)
        
        # æ ¹æ®ä¸åŒçš„æœç´¢æ¨¡å¼æ‰§è¡Œæ£€ç´¢
        if search_mode == "bm25":
            search_result = km.search_bm25(search_query, k=search_k, score_threshold=score_threshold)
        elif search_mode == "hybrid":
            vector_weight = state.get('vector_weight', 0.7)
            keyword_weight = state.get('keyword_weight', 0.3)
            search_result = km.search_hybrid(
                search_query, 
                k=search_k, 
                vector_weight=vector_weight, 
                keyword_weight=keyword_weight,
                score_threshold=score_threshold
            )
        else:
            # é»˜è®¤ä½¿ç”¨å‘é‡æ£€ç´¢
            search_result = km.search_with_details(search_query, k=search_k, score_threshold=score_threshold)
        
        # 4. å¤„ç†æ£€ç´¢ç»“æœ
        # æå–ç”¨äºå†™ä½œçš„ä¸Šä¸‹æ–‡æ–‡æœ¬
        knowledge_content = search_result.get("context", "")
        if not knowledge_content and "context_list" in search_result:
             # å¦‚æœ context å­—æ®µä¸ºç©ºï¼Œå°è¯•ä»åˆ—è¡¨æ‹¼æ¥
             knowledge_content = "\n".join([r.get("content", "") for r in search_result.get("context_list", [])])

        # æŒ‰ç« èŠ‚ç´¢å¼•ä¿å­˜æ£€ç´¢åˆ°çš„èƒŒæ™¯çŸ¥è¯†
        chapter_knowledge = state.get("chapter_knowledge", [])
        while len(chapter_knowledge) <= curr_idx:
            chapter_knowledge.append("")
        chapter_knowledge[curr_idx] = knowledge_content

        # è®°å½•æ£€ç´¢å†å²è®°å½•
        new_result_entry = {
            "chapter": curr_idx + 1,
            "title": chapter_title,
            "results_count": len(search_result.get("context_list", [])),
            "sources": [r.get("metadata", {}).get("filename", "æœªçŸ¥æ¥æº") for r in search_result.get("context_list", [])],
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        
        search_results = state.get("search_results", [])
        search_results.append(new_result_entry)
        
        logging.info(f"æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {new_result_entry['results_count']} æ¡ç›¸å…³è®°å½•")
        
        return {
            "knowledge_content": knowledge_content,
            "chapter_knowledge": chapter_knowledge,
            "search_results": search_results,
            "messages": [AIMessage(content=f"å·²ä¸ºç¬¬{curr_idx + 1}ç« æ£€ç´¢åˆ°ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚")],
            "last_successful_step": "retrieval"
        }
        
    except Exception as e:
        logging.error(f"çŸ¥è¯†æ£€ç´¢è¿‡ç¨‹å‡ºé”™: {str(e)}")
        # å®¹é”™å¤„ç†ï¼šæ£€ç´¢å¤±è´¥ä¸ä¸­æ–­æµç¨‹ï¼Œä½†æ¸…ç©ºæœ¬ç« èƒŒæ™¯çŸ¥è¯†
        return {
            "knowledge_content": "",
            "messages": [AIMessage(content=f"ç¬¬{curr_idx + 1}ç« çŸ¥è¯†æ£€ç´¢å¤±è´¥: {str(e)}ï¼Œå°†åŸºäºæ¨¡å‹è‡ªèº«çŸ¥è¯†å†™ä½œã€‚")],
            "last_successful_step": "retrieval_error"
        }

async def generate_chapter_node(state, config: RunnableConfig):
    """æ‰‹åŠ¨ç®¡ç†åˆ—è¡¨çš„ç”ŸæˆèŠ‚ç‚¹"""
    logging.info(f"--- âœï¸ ç”Ÿæˆç¬¬ {state.get('current_chapter', 0) + 1} ç« æ­£æ–‡ ---")
    
    # 1. åŸºç¡€å‚æ•°å‡†å¤‡
    curr_idx = state.get("current_chapter", 0)
    all_chapters = state.get("chapters", [])
    outline = state.get("outline", [])
    topic = state.get("topic", [])
    # state["topic"]
    chapter_info = outline[curr_idx] if curr_idx < len(outline) else {}
    
    chapter_title = chapter_info.get("title", f"ç¬¬{curr_idx + 1}ç« ")
    chapter_description = chapter_info.get("description", "")
    word_count = state.get("word_count", 300)
    
    # 1. æå–é…ç½®å¹¶è°ƒç”¨ LLM
    configurable = config.get("configurable", {})
    llm = get_llm(model=configurable.get("model_name"))
    
    # 3. å†™ä½œé£æ ¼ä¸æ ¼å¼åŒ–
    from Prompts.prompts import writing_prompt
    from Prompts.writing_styles import get_style_prompt_enhancement, normalize_style
    
    normalized_style = normalize_style(state.get("style", "academic"))
    style_enhancement = get_style_prompt_enhancement(normalized_style)
    unit = "å­—" if any(ord(c) > 127 for c in state.get("task", "")) else "words"

    # 4. è·å–ä¸Šä¸‹æ–‡ï¼ˆè¿è´¯æ€§æ§åˆ¶ï¼‰
    # è·å–ä¹‹å‰æ‰€æœ‰ç« èŠ‚çš„æ–‡æœ¬ï¼Œç”¨äºä¿æŒé€»è¾‘ä¸€è‡´
    previous_chapters_text = "\n\n".join(all_chapters[:curr_idx]) if all_chapters else "æ— å‰å‡ ç« å†…å®¹"
    
    # è·å–æœ¬ç« èŠ‚ä¸“é—¨æ£€ç´¢åˆ°çš„èƒŒæ™¯çŸ¥è¯†
    chapter_knowledge = state.get("chapter_knowledge", [])
    current_knowledge = chapter_knowledge[curr_idx] if curr_idx < len(chapter_knowledge) else state.get("knowledge_content", "")

    # 5. æ ¼å¼åŒ–åŸºç¡€ Prompt
    query = f"è¯·ä»¥ä¸‹é¢çš„æ–‡å­—ä¸ºé¢˜å†™æŠ¥å‘Šï¼š{topic}"
    try:
        prompt = writing_prompt.format(
            task=query,
            chapter_title=chapter_title,
            chapter_description=chapter_description,
            word_count=word_count,
            unit=unit,
            style_enhancement=style_enhancement,
            knowledge_content=current_knowledge,
            previous_chapters=previous_chapters_text
        )

        # # 6. å¤„ç†äººå·¥åé¦ˆ (Human Feedback Loop)
        # messages = state.get("messages", [])
        # feedback_applied = False
        
        # # é€†åºæŸ¥æ‰¾æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        # for i in range(len(messages) - 1, -1, -1):
        #     if messages[i].get("role") == "user":
        #         feedback = messages[i].get("content", "")
        #         if feedback and "äººå·¥åé¦ˆ" in feedback:
        #             logging.info(f"åº”ç”¨äººå·¥åé¦ˆåˆ°ç¬¬ {curr_idx + 1} ç« : {feedback}")
                    
        #             # å¦‚æœæ˜¯é‡å†™é€»è¾‘ï¼ŒåŠ å…¥å½“å‰ç« èŠ‚å·²æœ‰çš„è‰ç¨¿å†…å®¹
        #             if curr_idx < len(all_chapters) and all_chapters[curr_idx]:
        #                 prompt += f"\n\n## å½“å‰ç« èŠ‚è‰ç¨¿:\n{all_chapters[curr_idx]}\n"
                    
        #             prompt += f"\n\nã€é‡è¦æŒ‡ä»¤ã€‘:\n{feedback}\nè¯·æ ¹æ®æ­¤åé¦ˆè°ƒæ•´å†™ä½œã€‚"
                    
        #             # ç§»é™¤å·²ä½¿ç”¨çš„åé¦ˆæ¶ˆæ¯ï¼ˆé¿å…æ±¡æŸ“åç»­ç« èŠ‚ï¼‰
        #             messages.pop(i)
        #             feedback_applied = True
        #             break

    except KeyError as e:
        logging.error(f"Prompt æ ¼å¼åŒ–å¤±è´¥: {e}")
        raise ValueError(f"Missing prompt variable: {e}")
    
    # 7. æ‰§è¡Œ LLM ç”Ÿæˆ
    response = await llm.ainvoke(prompt)
    content = response.content.strip()
    # print()
    logging.info(f"--- ç”Ÿæˆç¬¬ {state.get('current_chapter', 0) + 1} ç« æ­£æ–‡ ---\n {content}")
    
    # 2. æ‰‹åŠ¨ç®¡ç†åˆ—è¡¨æ›´æ–°
    while len(all_chapters) <= curr_idx:
        all_chapters.append("")
    
    # æ›¿æ¢å½“å‰ç« èŠ‚å†…å®¹
    all_chapters[curr_idx] = content
    
    # 3. ä¿å­˜ç« èŠ‚è¯¦ç»†ä¿¡æ¯ï¼ˆé¢˜ç›® + å†…å®¹ï¼‰
    chapter_details = state.get("chapter_details", [])
    while len(chapter_details) <= curr_idx:
        chapter_details.append({"title": "", "content": ""})
    
    chapter_details[curr_idx] = {
        "title": chapter_title,
        "content": content
    }

    # 4. è¿”å›æ›´æ–°åçš„å®Œæ•´ State
    return {
        "chapters": all_chapters,
        "chapter_details": chapter_details,
        "current_chapter": curr_idx + 1, # ç´¢å¼•æ¨è¿›
        "messages": [{"role": "assistant", "content": f"ç¬¬{curr_idx+1}ç« ç”ŸæˆæˆåŠŸ"}],
        "last_successful_step": "writing"
    }


async def merge_article_node(state, config: RunnableConfig):
    """åˆå¹¶æ‰€æœ‰ç« èŠ‚ä¸ºå®Œæ•´çš„ Markdown æ–‡æ¡£"""
    logging.info("--- ğŸ“„ åˆå¹¶æ–‡ç« èŠ‚ç‚¹ ---")
    
    try:
        # è·å–åŸºæœ¬ä¿¡æ¯
        topic = state.get("topic", "æœªå‘½åæ–‡ç« ")
        chapter_details = state.get("chapter_details", [])
        outline = state.get("outline", [])
        
        # å¦‚æœæ²¡æœ‰ chapter_detailsï¼Œä½¿ç”¨ chapters å’Œ outline
        if not chapter_details:
            chapters = state.get("chapters", [])
            chapter_details = []
            for idx, content in enumerate(chapters):
                if idx < len(outline):
                    title = outline[idx].get("title", f"ç¬¬{idx+1}ç« ")
                else:
                    title = f"ç¬¬{idx+1}ç« "
                chapter_details.append({"title": title, "content": content})
        
        # æ„å»º Markdown æ–‡æ¡£
        markdown_content = []
        
        # 1. æ·»åŠ æ–‡ç« æ ‡é¢˜
        markdown_content.append(f"# {topic}\n")
        
        # 2. æ·»åŠ ç”Ÿæˆä¿¡æ¯
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        markdown_content.append(f"*ç”Ÿæˆæ—¶é—´: {timestamp}*\n")
        markdown_content.append(f"*æ€»ç« èŠ‚æ•°: {len(chapter_details)}*\n")
        markdown_content.append("---\n")
        
        # 3. å¯é€‰ï¼šæ·»åŠ ç›®å½•
        if len(chapter_details) > 1:
            markdown_content.append("## ç›®å½•\n")
            for idx, detail in enumerate(chapter_details, 1):
                title = detail.get("title", f"ç¬¬{idx}ç« ")
                # ç”Ÿæˆé”šç‚¹é“¾æ¥ï¼ˆMarkdown æ ¼å¼ï¼‰
                anchor = title.replace(" ", "-").lower()
                markdown_content.append(f"{idx}. [{title}](#{anchor})\n")
            markdown_content.append("\n---\n")
        
        # 4. æ·»åŠ æ‰€æœ‰ç« èŠ‚å†…å®¹
        for idx, detail in enumerate(chapter_details, 1):
            title = detail.get("title", f"ç¬¬{idx}ç« ")
            content = detail.get("content", "")
            
            # ç« èŠ‚æ ‡é¢˜ï¼ˆä½¿ç”¨äºŒçº§æ ‡é¢˜ï¼‰
            markdown_content.append(f"\n## {title}\n")
            
            # ç« èŠ‚å†…å®¹
            markdown_content.append(f"{content}\n")
            
            # ç« èŠ‚åˆ†éš”ç¬¦ï¼ˆé™¤äº†æœ€åä¸€ç« ï¼‰
            if idx < len(chapter_details):
                markdown_content.append("\n---\n")
        
        # 5. åˆå¹¶æ‰€æœ‰å†…å®¹
        merged_article = "\n".join(markdown_content)
        
        logging.info(f"æ–‡ç« åˆå¹¶å®Œæˆï¼Œæ€»é•¿åº¦: {len(merged_article)} å­—ç¬¦")
        
        # 6. å¯é€‰ï¼šä¿å­˜åˆ°æ–‡ä»¶
        # import os
        # output_dir = "output"
        # os.makedirs(output_dir, exist_ok=True)
        # filename = f"{output_dir}/{topic.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        # with open(filename, "w", encoding="utf-8") as f:
        #     f.write(merged_article)
        # logging.info(f"æ–‡ç« å·²ä¿å­˜åˆ°: {filename}")
        
        return {
            "merged_article": merged_article,
            "final_content": merged_article,  # å…¼å®¹æ—§å­—æ®µ
            "messages": [{"role": "assistant", "content": f"æ–‡ç« åˆå¹¶å®Œæˆï¼Œå…± {len(chapter_details)} ç« èŠ‚"}],
            "last_successful_step": "merge"
        }
    
    except Exception as e:
        logging.error(f"åˆå¹¶æ–‡ç« å¤±è´¥: {str(e)}")
        return {
            "messages": [{"role": "assistant", "content": f"åˆå¹¶æ–‡ç« æ—¶å‡ºé”™: {str(e)}"}],
            "last_successful_step": "merge_error"
        }