import re 
import gradio as gr
import asyncio
import hashlib
from uuid import UUID
from langgraph_sdk import get_client
hostname = "http://langgraph-api-learn-2026-pre1231:2024"
# GRAPH_ID = "my_agent"
if hostname is None:
    API_URL = "http://127.0.0.1:2024"
else:
    API_URL = hostname

GRAPH_ID = "my_agent"

def name_to_uuid(name: str) -> str:
    """å°†æ™®é€šå­—ç¬¦ä¸²è½¬ä¸º 0.5.39 ç‰ˆæœ¬å¼ºåˆ¶è¦æ±‚çš„ UUID æ ¼å¼"""
    hash_obj = hashlib.md5(name.encode('utf-8'))
    return str(UUID(hash_obj.hexdigest()))

def format_ai_response(text: str) -> str:
    """
    å¦‚æœå­˜åœ¨ <think> æ ‡ç­¾ï¼Œå°†å…¶åŒ…è£…åœ¨æŠ˜å æ¡†å†…ï¼›
    å¦‚æœæ²¡æœ‰ï¼Œåˆ™ç›´æ¥è¾“å‡ºç»“æœã€‚
    """
    if not text: return ""
    
    pattern = r'<(?:think|thought)>(.*?)</(?:think|thought)>'
    match = re.search(pattern, text, flags=re.DOTALL)
    
    if match:
        thought_content = match.group(1).strip()
        # ç§»é™¤æ­£æ–‡ä¸­çš„ think éƒ¨åˆ†
        answer_content = re.sub(pattern, '', text, flags=re.DOTALL).strip()
        return f"<details><summary><b>ğŸ” æ€è€ƒè¿‡ç¨‹ (ç‚¹å‡»å±•å¼€)</b></summary>\n\n{thought_content}\n\n</details>\n\n{answer_content}"
    
    return text


async def ensure_thread_exists(client, thread_id):
    """ç‹¬ç«‹åŠŸèƒ½ï¼šç¡®ä¿çº¿ç¨‹å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º"""
    try:
        await client.threads.get(thread_id)
    except Exception:
        # å¦‚æœè·å–å¤±è´¥ï¼ˆ404ï¼‰ï¼Œåˆ™æ‰‹åŠ¨åˆ›å»º
        # 2026-01-04T05:38:12.152238Z [warning  ] POST /threads/40b9ce26-4c12-cd34-5f55-a803f9cdcfae/runs/stream 404 2ms [langgraph_api.server] api_revision=212ad47 api_variant=local_dev langgraph_api_version=0.5.39 latency_ms=2 method=POST path=/threads/{thread_id}/runs/stream path_params={'thread_id': '40b9ce26-4c12-cd34-5f55-a803f9cdcfae'} proto=1.1 query_string= req_header={} request_id=dc850bd1-3dff-45da-9b50-e150b25509f3 res_header={} route=/threads/{thread_id}/runs/stream status=404 thread_name=MainThread
        await client.threads.create(thread_id=thread_id)
        print(f"DEBUG: Created new thread: {thread_id}")


def extract_content_from_event(data):
    """
    ç‹¬ç«‹åŠŸèƒ½ï¼šä»ä¸åŒçš„æ•°æ®ç»“æ„ä¸­æå–æ–‡æœ¬å†…å®¹
    """
    content = ""
    if isinstance(data, list):
        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œé€šå¸¸åŒ…å«å¤šæ¡æ¶ˆæ¯æˆ–æ¶ˆæ¯ç‰‡æ®µ
        for m in data:
            if isinstance(m, dict) and "content" in m:
                content += m["content"]
            elif hasattr(m, "content"):
                content += m.content
    elif isinstance(data, dict):
        content = data.get("content", "")
    elif hasattr(data, "content"):
        content = data.content
    return content
    
# --- 2. æ ¸å¿ƒé¢„æµ‹é€»è¾‘ ---
async def predict(message, history, task_context, session_id, file_obj):
    """
    message: å½“å‰ç”¨æˆ·çš„å…·ä½“æé—® (æ¥è‡ª ChatInterface)
    history: è‡ªåŠ¨ç»´æŠ¤çš„å¯¹è¯å†å²
    task_context: å¾…åˆ†æçš„æ–‡ç« /èƒŒæ™¯å†…å®¹ (æ¥è‡ªç‹¬ç«‹çš„ Textbox)
    """
    client = get_client(url=API_URL)
    thread_id = name_to_uuid(session_id)
    
    # ç¡®ä¿çº¿ç¨‹å­˜åœ¨
    try:
        await client.threads.get(thread_id)
    except:
        await client.threads.create(thread_id=thread_id)

    # æ ¸å¿ƒä¿®æ”¹ï¼šåŒºåˆ† task å’Œ messages
    input_state = {
        "task": task_context,  # è¿™é‡Œæ”¾æ–‡ç« åŸæ–‡æˆ–èƒŒæ™¯
        "messages": [{"role": "user", "content": message}] # è¿™é‡Œæ”¾å½“å‰ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤
    }
    
    if file_obj is not None:
        input_state["files"] = [file_obj.name]

    msg_cache = {}
    try:
        async for event in client.runs.stream(
            thread_id,
            GRAPH_ID,
            input=input_state,
            stream_mode="values", 
        ):
            if event.event == "metadata" or not event.data:
                continue
            
            data = event.data
            messages = data.get("messages", []) if isinstance(data, dict) else data
            
            if not messages: continue
            
            current_msg = messages[-1]
            msg_id = getattr(current_msg, "id", "default")
            if isinstance(current_msg, dict): msg_id = current_msg.get("id", "default")
            
            # æå–å†…å®¹ (å…¼å®¹å¤„ç†)
            content = ""
            if isinstance(current_msg, dict): content = current_msg.get("content", "")
            else: content = getattr(current_msg, "content", "")
            
            msg_cache[msg_id] = content
            full_raw_text = "".join(msg_cache.values())
            
            yield format_ai_response(full_raw_text)
            
    except Exception as e:
        yield f"âŒ è¿è¡Œå¼‚å¸¸: {str(e)}"

def create_ui():
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ“‘ AI æ·±åº¦æŠ¥å‘Šåˆ†æåŠ©æ‰‹")
        
        with gr.Row():
            # å·¦ä¾§é…ç½®åŒº
            with gr.Column(scale=1):
                session_id = gr.Textbox(label="ä¼šè¯ ID", value="user_session_01")
                file_upload = gr.File(label="ä¸Šä¼ å‚è€ƒæ–‡æ¡£")
                # è¿™é‡Œçš„ task_context å¯¹åº”ä½ è¦æ±‚çš„ state["task"]
                task_context = gr.Textbox(
                    label="å¾…åˆ†æçš„æ–‡ç« /èƒŒæ™¯å†…å®¹", 
                    placeholder="åœ¨æ­¤ç²˜è´´é•¿ç¯‡æ–‡ç« ã€æ•°æ®æˆ–èƒŒæ™¯èµ„æ–™...",
                    lines=15
                )
            
            # å³ä¾§å¯¹è¯åŒº
            with gr.Column(scale=2):
                # ä½¿ç”¨ ChatInterface å¯ä»¥è‡ªåŠ¨å¤„ç† history é€»è¾‘
                chat = gr.ChatInterface(
                    fn=predict,
                    additional_inputs=[task_context, session_id, file_upload],
                    #type="messages" # ä½¿ç”¨æ–°çš„ messages æ ¼å¼
                )
                
    return demo

if __name__ == "__main__":
    # å¯åŠ¨ Gradio
    ui = create_ui()
    ui.launch(
        server_name="0.0.0.0",
        server_port=7860,
    )